{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection - simplistic greedy approach\n",
    "As we've now seen, it's fairly easy to overfit a model and as such we may need to make decisions about what variables or factors to include in the model and which to leave out. A simplistic way to do this is to add features individually, one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Split the data into a test and train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53617\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>CAT_Insurer</th>\n",
       "      <th>CAT_Region_Num</th>\n",
       "      <th>205d_V2_CUR</th>\n",
       "      <th>205d_V3_PRC</th>\n",
       "      <th>212d_V1_CUR</th>\n",
       "      <th>212d_V2_PRC</th>\n",
       "      <th>213d_V2_CUR</th>\n",
       "      <th>213d_V3_PRC</th>\n",
       "      <th>...</th>\n",
       "      <th>KG_SPS_226d_V1_CUR</th>\n",
       "      <th>KG_SPS_227d_V1_PRC</th>\n",
       "      <th>KG_SPS_229d_V1_CUR</th>\n",
       "      <th>KG_SX_226d_V1_CUR</th>\n",
       "      <th>KG_SX_227d_V1_PRC</th>\n",
       "      <th>KG_SX_229d_V1_CUR</th>\n",
       "      <th>KG_TOT_226d_V1_CUR</th>\n",
       "      <th>KG_TOT_227d_V1_PRC</th>\n",
       "      <th>KG_TOT_229d_V1_CUR</th>\n",
       "      <th>Premium</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.326051</td>\n",
       "      <td>0.215957</td>\n",
       "      <td>0.322181</td>\n",
       "      <td>0.234051</td>\n",
       "      <td>0.377469</td>\n",
       "      <td>0.213745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.275441</td>\n",
       "      <td>0.373868</td>\n",
       "      <td>0.285167</td>\n",
       "      <td>0.149472</td>\n",
       "      <td>0.610291</td>\n",
       "      <td>0.131174</td>\n",
       "      <td>0.326051</td>\n",
       "      <td>0.215957</td>\n",
       "      <td>0.359147</td>\n",
       "      <td>0.409432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.326051</td>\n",
       "      <td>0.215957</td>\n",
       "      <td>0.322181</td>\n",
       "      <td>0.234051</td>\n",
       "      <td>0.377469</td>\n",
       "      <td>0.213745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.275441</td>\n",
       "      <td>0.373868</td>\n",
       "      <td>0.285167</td>\n",
       "      <td>0.149472</td>\n",
       "      <td>0.610291</td>\n",
       "      <td>0.131174</td>\n",
       "      <td>0.326051</td>\n",
       "      <td>0.215957</td>\n",
       "      <td>0.359147</td>\n",
       "      <td>0.394941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.326051</td>\n",
       "      <td>0.215957</td>\n",
       "      <td>0.322181</td>\n",
       "      <td>0.234051</td>\n",
       "      <td>0.377469</td>\n",
       "      <td>0.213745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.275441</td>\n",
       "      <td>0.373868</td>\n",
       "      <td>0.285167</td>\n",
       "      <td>0.149472</td>\n",
       "      <td>0.610291</td>\n",
       "      <td>0.131174</td>\n",
       "      <td>0.326051</td>\n",
       "      <td>0.215957</td>\n",
       "      <td>0.359147</td>\n",
       "      <td>0.358463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.326051</td>\n",
       "      <td>0.215957</td>\n",
       "      <td>0.322181</td>\n",
       "      <td>0.234051</td>\n",
       "      <td>0.377469</td>\n",
       "      <td>0.213745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.275441</td>\n",
       "      <td>0.373868</td>\n",
       "      <td>0.285167</td>\n",
       "      <td>0.149472</td>\n",
       "      <td>0.610291</td>\n",
       "      <td>0.131174</td>\n",
       "      <td>0.326051</td>\n",
       "      <td>0.215957</td>\n",
       "      <td>0.359147</td>\n",
       "      <td>0.321986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.326051</td>\n",
       "      <td>0.215957</td>\n",
       "      <td>0.322181</td>\n",
       "      <td>0.234051</td>\n",
       "      <td>0.377469</td>\n",
       "      <td>0.213745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.275441</td>\n",
       "      <td>0.373868</td>\n",
       "      <td>0.285167</td>\n",
       "      <td>0.149472</td>\n",
       "      <td>0.610291</td>\n",
       "      <td>0.131174</td>\n",
       "      <td>0.326051</td>\n",
       "      <td>0.215957</td>\n",
       "      <td>0.359147</td>\n",
       "      <td>0.285634</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 196 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        ID  CAT_Insurer  CAT_Region_Num  205d_V2_CUR  \\\n",
       "0           0  0.000000          0.0             0.0     0.326051   \n",
       "1           1  0.000019          0.0             0.0     0.326051   \n",
       "2           2  0.000037          0.0             0.0     0.326051   \n",
       "3           3  0.000056          0.0             0.0     0.326051   \n",
       "4           4  0.000075          0.0             0.0     0.326051   \n",
       "\n",
       "   205d_V3_PRC  212d_V1_CUR  212d_V2_PRC  213d_V2_CUR  213d_V3_PRC    ...     \\\n",
       "0     0.215957     0.322181     0.234051     0.377469     0.213745    ...      \n",
       "1     0.215957     0.322181     0.234051     0.377469     0.213745    ...      \n",
       "2     0.215957     0.322181     0.234051     0.377469     0.213745    ...      \n",
       "3     0.215957     0.322181     0.234051     0.377469     0.213745    ...      \n",
       "4     0.215957     0.322181     0.234051     0.377469     0.213745    ...      \n",
       "\n",
       "   KG_SPS_226d_V1_CUR  KG_SPS_227d_V1_PRC  KG_SPS_229d_V1_CUR  \\\n",
       "0            0.275441            0.373868            0.285167   \n",
       "1            0.275441            0.373868            0.285167   \n",
       "2            0.275441            0.373868            0.285167   \n",
       "3            0.275441            0.373868            0.285167   \n",
       "4            0.275441            0.373868            0.285167   \n",
       "\n",
       "   KG_SX_226d_V1_CUR  KG_SX_227d_V1_PRC  KG_SX_229d_V1_CUR  \\\n",
       "0           0.149472           0.610291           0.131174   \n",
       "1           0.149472           0.610291           0.131174   \n",
       "2           0.149472           0.610291           0.131174   \n",
       "3           0.149472           0.610291           0.131174   \n",
       "4           0.149472           0.610291           0.131174   \n",
       "\n",
       "   KG_TOT_226d_V1_CUR  KG_TOT_227d_V1_PRC  KG_TOT_229d_V1_CUR   Premium  \n",
       "0            0.326051            0.215957            0.359147  0.409432  \n",
       "1            0.326051            0.215957            0.359147  0.394941  \n",
       "2            0.326051            0.215957            0.359147  0.358463  \n",
       "3            0.326051            0.215957            0.359147  0.321986  \n",
       "4            0.326051            0.215957            0.359147  0.285634  \n",
       "\n",
       "[5 rows x 196 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('Swiss_Healthcare_Premium_Prediction.csv.gz', compression='gzip')\n",
    "\n",
    "df = df.fillna(value=0)\n",
    "X = df[df.columns[:-1]]\n",
    "y = df['Premium']\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train , X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Find the [single] best feature to train a regression model on\n",
    "Loop through all of the X features and train an unpenalized LinearRegression model using each of those single features. Find the feature that produces the lowest Mean squared test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "linreg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The single best predictor was: KG_TOT_229d_V1_CUR\n"
     ]
    }
   ],
   "source": [
    "#Your code here\n",
    "from sklearn.linear_model import LinearRegression\n",
    "min_test_err = 10**100 #Create an incredibly high initialization val\n",
    "best_feat = None\n",
    "\n",
    "def mse(residual_col):\n",
    "    return np.mean(residual_col.astype(float).map(lambda x: x**2))\n",
    "\n",
    "for feat in X.columns:\n",
    "    linreg = LinearRegression()\n",
    "    cur_X_train = np.array(X_train[feat]).reshape(-1, 1)\n",
    "    cur_X_test = np.array(X_test[feat]).reshape(-1, 1)\n",
    "    linreg.fit(cur_X_train, y_train)\n",
    "    y_hat_test = linreg.predict(cur_X_test)\n",
    "    test_err = mse(y_hat_test-y_test)\n",
    "#     print(feat, round(test_err,2))\n",
    "    if  test_err < min_test_err:\n",
    "        min_test_err = test_err\n",
    "        best_feat = feat\n",
    "print('The single best predictor was: {}'.format(feat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generalize #2\n",
    "Write a function that takes in a desired number of features and returns a model using the top n features (according to test set error). Be sure to do this iteratively. In other words, rather then simply taking the top n features based on how well each performs individually, first find the best feature and train a model, then loop back through all of the remaining features and select that which produces the best results in combination with the best feature already selected. Continue on finding the best third feature in combination with the previous 2 features, etc. This process will continue until you reach the desired number of features (or there are no features left)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_feat(X_train, X_test, y_train, y_test, feat_options, prev_feats=[]):\n",
    "        min_test_err = 10**100 #Create an incredibly high initialization val\n",
    "        best_feat = None\n",
    "        for feat in feat_options:\n",
    "            linreg = LinearRegression()\n",
    "            if prev_feats == []:\n",
    "                cur_X_train = np.array(X_train[feat]).reshape(-1, 1)\n",
    "                cur_X_test = np.array(X_test[feat]).reshape(-1, 1)\n",
    "            else:\n",
    "                feats = prev_feats + [feat]\n",
    "                cur_X_train = X_train[feats]\n",
    "                cur_X_test = X_test[feats]\n",
    "            linreg.fit(cur_X_train, y_train)\n",
    "            y_hat_test = linreg.predict(cur_X_test)\n",
    "            test_err = mse(y_hat_test-y_test)\n",
    "            if  test_err < min_test_err:\n",
    "                min_test_err = test_err\n",
    "                best_feat = feat\n",
    "        return best_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg_greedy_feat(n_feats, X_train, X_test, y_train, y_test):\n",
    "    #Your code here\n",
    "    cur_model_feats = []\n",
    "    remaining_feats = list(X.columns)\n",
    "    for n in range(1,n_feats+1):\n",
    "        next_feat = best_feat(X_train, X_test, y_train, y_test,\n",
    "                              feat_options=remaining_feats, prev_feats = cur_model_feats)\n",
    "        cur_model_feats.append(next_feat)\n",
    "        remaining_feats.remove(next_feat)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train[cur_model_feats], y_train)\n",
    "    return model, cur_model_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Learning Curves\n",
    "Iterate from 2 to 20 feature variables. Use your greedy classifier defined above to generate a linear regression model with successively more and more features incorporated into the model. Then plot the train and test errors as a function of the number of variables incorporated into each of these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'MSE versus Number of Features Incorporated into Model')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#***********Warning this block will take several minutes to run************\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "for i in range(2,21):\n",
    "    cycle_start = datetime.datetime.now()\n",
    "    #print('On iteration: {}'.format(i-1))\n",
    "    #Train Greedy Classifier Model with this many features\n",
    "    model, cur_model_feats = linreg_greedy_feat(i, X_train, X_test, y_train, y_test)\n",
    "    model.fit(X_train[cur_model_feats], y_train)\n",
    "    \n",
    "    #Calculate Training Mean Squared Error\n",
    "    y_hat_train = model.predict(X_train[cur_model_feats])\n",
    "    train_err = mse(y_hat_train-y_train)\n",
    "    \n",
    "    #Calculate Test Mean Squared Error\n",
    "    y_hat_test = model.predict(X_test[cur_model_feats])\n",
    "    test_err = mse(y_hat_test-y_test)\n",
    "    \n",
    "    #Plot Results\n",
    "    if i ==2:\n",
    "        plt.scatter(i, train_err, c='red', label='Train Error')\n",
    "        plt.scatter(i, test_err, c='orange', label='Test Error')\n",
    "    else:\n",
    "        plt.scatter(i, train_err, c='red')\n",
    "        plt.scatter(i, test_err, c='orange')\n",
    "    end = datetime.datetime.now()\n",
    "    cycle_time = end - cycle_start\n",
    "    elapsed = end - start\n",
    "#     print('Cycle took: {}'.format(cycle_time))\n",
    "#     print('Total time elapsed: {}'.format(elapsed))\n",
    "\n",
    "#Add Legend and Descriptive Title/Axis Labels\n",
    "plt.legend(bbox_to_anchor=(1,1))\n",
    "plt.ylim(0, 0.01)\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('Number of Features in Model')\n",
    "plt.title('MSE versus Number of Features Incorporated into Model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

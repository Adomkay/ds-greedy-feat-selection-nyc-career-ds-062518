{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection - simplistic greedy approach\n",
    "As we've now seen, it's fairly easy to overfit a model and as such we may need to make decisions about what variables or factors to include in the model and which to leave out. A simplistic way to do this is to add features individually, one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Split the data into a test and train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53617\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>CAT_Insurer</th>\n",
       "      <th>CAT_Region_Num</th>\n",
       "      <th>205d_V2_CUR</th>\n",
       "      <th>205d_V3_PRC</th>\n",
       "      <th>212d_V1_CUR</th>\n",
       "      <th>212d_V2_PRC</th>\n",
       "      <th>213d_V2_CUR</th>\n",
       "      <th>213d_V3_PRC</th>\n",
       "      <th>...</th>\n",
       "      <th>KG_SPS_226d_V1_CUR</th>\n",
       "      <th>KG_SPS_227d_V1_PRC</th>\n",
       "      <th>KG_SPS_229d_V1_CUR</th>\n",
       "      <th>KG_SX_226d_V1_CUR</th>\n",
       "      <th>KG_SX_227d_V1_PRC</th>\n",
       "      <th>KG_SX_229d_V1_CUR</th>\n",
       "      <th>KG_TOT_226d_V1_CUR</th>\n",
       "      <th>KG_TOT_227d_V1_PRC</th>\n",
       "      <th>KG_TOT_229d_V1_CUR</th>\n",
       "      <th>Premium</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.326051</td>\n",
       "      <td>0.215957</td>\n",
       "      <td>0.322181</td>\n",
       "      <td>0.234051</td>\n",
       "      <td>0.377469</td>\n",
       "      <td>0.213745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.275441</td>\n",
       "      <td>0.373868</td>\n",
       "      <td>0.285167</td>\n",
       "      <td>0.149472</td>\n",
       "      <td>0.610291</td>\n",
       "      <td>0.131174</td>\n",
       "      <td>0.326051</td>\n",
       "      <td>0.215957</td>\n",
       "      <td>0.359147</td>\n",
       "      <td>0.409432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.326051</td>\n",
       "      <td>0.215957</td>\n",
       "      <td>0.322181</td>\n",
       "      <td>0.234051</td>\n",
       "      <td>0.377469</td>\n",
       "      <td>0.213745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.275441</td>\n",
       "      <td>0.373868</td>\n",
       "      <td>0.285167</td>\n",
       "      <td>0.149472</td>\n",
       "      <td>0.610291</td>\n",
       "      <td>0.131174</td>\n",
       "      <td>0.326051</td>\n",
       "      <td>0.215957</td>\n",
       "      <td>0.359147</td>\n",
       "      <td>0.394941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.326051</td>\n",
       "      <td>0.215957</td>\n",
       "      <td>0.322181</td>\n",
       "      <td>0.234051</td>\n",
       "      <td>0.377469</td>\n",
       "      <td>0.213745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.275441</td>\n",
       "      <td>0.373868</td>\n",
       "      <td>0.285167</td>\n",
       "      <td>0.149472</td>\n",
       "      <td>0.610291</td>\n",
       "      <td>0.131174</td>\n",
       "      <td>0.326051</td>\n",
       "      <td>0.215957</td>\n",
       "      <td>0.359147</td>\n",
       "      <td>0.358463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.326051</td>\n",
       "      <td>0.215957</td>\n",
       "      <td>0.322181</td>\n",
       "      <td>0.234051</td>\n",
       "      <td>0.377469</td>\n",
       "      <td>0.213745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.275441</td>\n",
       "      <td>0.373868</td>\n",
       "      <td>0.285167</td>\n",
       "      <td>0.149472</td>\n",
       "      <td>0.610291</td>\n",
       "      <td>0.131174</td>\n",
       "      <td>0.326051</td>\n",
       "      <td>0.215957</td>\n",
       "      <td>0.359147</td>\n",
       "      <td>0.321986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.326051</td>\n",
       "      <td>0.215957</td>\n",
       "      <td>0.322181</td>\n",
       "      <td>0.234051</td>\n",
       "      <td>0.377469</td>\n",
       "      <td>0.213745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.275441</td>\n",
       "      <td>0.373868</td>\n",
       "      <td>0.285167</td>\n",
       "      <td>0.149472</td>\n",
       "      <td>0.610291</td>\n",
       "      <td>0.131174</td>\n",
       "      <td>0.326051</td>\n",
       "      <td>0.215957</td>\n",
       "      <td>0.359147</td>\n",
       "      <td>0.285634</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 196 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        ID  CAT_Insurer  CAT_Region_Num  205d_V2_CUR  \\\n",
       "0           0  0.000000          0.0             0.0     0.326051   \n",
       "1           1  0.000019          0.0             0.0     0.326051   \n",
       "2           2  0.000037          0.0             0.0     0.326051   \n",
       "3           3  0.000056          0.0             0.0     0.326051   \n",
       "4           4  0.000075          0.0             0.0     0.326051   \n",
       "\n",
       "   205d_V3_PRC  212d_V1_CUR  212d_V2_PRC  213d_V2_CUR  213d_V3_PRC    ...     \\\n",
       "0     0.215957     0.322181     0.234051     0.377469     0.213745    ...      \n",
       "1     0.215957     0.322181     0.234051     0.377469     0.213745    ...      \n",
       "2     0.215957     0.322181     0.234051     0.377469     0.213745    ...      \n",
       "3     0.215957     0.322181     0.234051     0.377469     0.213745    ...      \n",
       "4     0.215957     0.322181     0.234051     0.377469     0.213745    ...      \n",
       "\n",
       "   KG_SPS_226d_V1_CUR  KG_SPS_227d_V1_PRC  KG_SPS_229d_V1_CUR  \\\n",
       "0            0.275441            0.373868            0.285167   \n",
       "1            0.275441            0.373868            0.285167   \n",
       "2            0.275441            0.373868            0.285167   \n",
       "3            0.275441            0.373868            0.285167   \n",
       "4            0.275441            0.373868            0.285167   \n",
       "\n",
       "   KG_SX_226d_V1_CUR  KG_SX_227d_V1_PRC  KG_SX_229d_V1_CUR  \\\n",
       "0           0.149472           0.610291           0.131174   \n",
       "1           0.149472           0.610291           0.131174   \n",
       "2           0.149472           0.610291           0.131174   \n",
       "3           0.149472           0.610291           0.131174   \n",
       "4           0.149472           0.610291           0.131174   \n",
       "\n",
       "   KG_TOT_226d_V1_CUR  KG_TOT_227d_V1_PRC  KG_TOT_229d_V1_CUR   Premium  \n",
       "0            0.326051            0.215957            0.359147  0.409432  \n",
       "1            0.326051            0.215957            0.359147  0.394941  \n",
       "2            0.326051            0.215957            0.359147  0.358463  \n",
       "3            0.326051            0.215957            0.359147  0.321986  \n",
       "4            0.326051            0.215957            0.359147  0.285634  \n",
       "\n",
       "[5 rows x 196 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('Swiss_Healthcare_Premium_Prediction.csv.gz', compression='gzip')\n",
    "\n",
    "df = df.fillna(value=0)\n",
    "X = df[df.columns[:-1]]\n",
    "y = df['Premium']\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train , X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Find the [single] best feature to train a regression model on\n",
    "Loop through all of the X features and train an unpenalized LinearRegression model using each of those single features. Find the feature that produces the lowest Mean squared test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "linreg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The single best predictor was: KG_TOT_229d_V1_CUR\n"
     ]
    }
   ],
   "source": [
    "#Your code here\n",
    "from sklearn.linear_model import LinearRegression\n",
    "min_test_err = 10**100 #Create an incredibly high initialization val\n",
    "best_feat = None\n",
    "\n",
    "def mse(residual_col):\n",
    "    return np.mean(residual_col.astype(float).map(lambda x: x**2))\n",
    "\n",
    "for feat in X.columns:\n",
    "    linreg = LinearRegression()\n",
    "    cur_X_train = np.array(X_train[feat]).reshape(-1, 1)\n",
    "    cur_X_test = np.array(X_test[feat]).reshape(-1, 1)\n",
    "    linreg.fit(cur_X_train, y_train)\n",
    "    y_hat_test = linreg.predict(cur_X_test)\n",
    "    test_err = mse(y_hat_test-y_test)\n",
    "#     print(feat, round(test_err,2))\n",
    "    if  test_err < min_test_err:\n",
    "        min_test_err = test_err\n",
    "        best_feat = feat\n",
    "print('The single best predictor was: {}'.format(feat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generalize #2\n",
    "Write a function that takes in a desired number of features and returns a model using the top n features (according to test set error). Be sure to do this iteratively. In other words, rather then simply taking the top n features based on how well each performs individually, first find the best feature and train a model, then loop back through all of the remaining features and select that which produces the best results in combination with the best feature already selected. Continue on finding the best third feature in combination with the previous 2 features, etc. This process will continue until you reach the desired number of features (or there are no features left)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_feat(X_train, X_test, y_train, y_test, feat_options, prev_feats=[]):\n",
    "        min_test_err = 10**100 #Create an incredibly high initialization val\n",
    "        best_feat = None\n",
    "        for feat in feat_options:\n",
    "            linreg = LinearRegression()\n",
    "            if prev_feats == []:\n",
    "                cur_X_train = np.array(X_train[feat]).reshape(-1, 1)\n",
    "                cur_X_test = np.array(X_test[feat]).reshape(-1, 1)\n",
    "            else:\n",
    "                feats = prev_feats + [feat]\n",
    "                cur_X_train = X_train[feats]\n",
    "                cur_X_test = X_test[feats]\n",
    "            linreg.fit(cur_X_train, y_train)\n",
    "            y_hat_test = linreg.predict(cur_X_test)\n",
    "            test_err = mse(y_hat_test-y_test)\n",
    "            if  test_err < min_test_err:\n",
    "                min_test_err = test_err\n",
    "                best_feat = feat\n",
    "        return best_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg_greedy_feat(n_feats, X_train, X_test, y_train, y_test):\n",
    "    #Your code here\n",
    "    cur_model_feats = []\n",
    "    remaining_feats = list(X.columns)\n",
    "    for n in range(1,n_feats+1):\n",
    "        next_feat = best_feat(X_train, X_test, y_train, y_test,\n",
    "                              feat_options=remaining_feats, prev_feats = cur_model_feats)\n",
    "        cur_model_feats.append(next_feat)\n",
    "        remaining_feats.remove(next_feat)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train[cur_model_feats], y_train)\n",
    "    return model, cur_model_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Learning Curves\n",
    "Iterate from 2 to 20 feature variables. Use your greedy classifier defined above to generate a linear regression model with successively more and more features incorporated into the model. Then plot the train and test errors as a function of the number of variables incorporated into each of these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'MSE versus Number of Features Incorporated into Model')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a29a340b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#***********Warning this block will take several minutes to run************\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "for i in range(2,21):\n",
    "    cycle_start = datetime.datetime.now()\n",
    "    #print('On iteration: {}'.format(i-1))\n",
    "    #Train Greedy Classifier Model with this many features\n",
    "    model, cur_model_feats = linreg_greedy_feat(i, X_train, X_test, y_train, y_test)\n",
    "    model.fit(X_train[cur_model_feats], y_train)\n",
    "    \n",
    "    #Calculate Training Mean Squared Error\n",
    "    y_hat_train = model.predict(X_train[cur_model_feats])\n",
    "    train_err = mse(y_hat_train-y_train)\n",
    "    \n",
    "    #Calculate Test Mean Squared Error\n",
    "    y_hat_test = model.predict(X_test[cur_model_feats])\n",
    "    test_err = mse(y_hat_test-y_test)\n",
    "    \n",
    "    #Plot Results\n",
    "    if i ==2:\n",
    "        plt.scatter(i, train_err, c='red', label='Train Error')\n",
    "        plt.scatter(i, test_err, c='orange', label='Test Error')\n",
    "    else:\n",
    "        plt.scatter(i, train_err, c='red')\n",
    "        plt.scatter(i, test_err, c='orange')\n",
    "    end = datetime.datetime.now()\n",
    "    cycle_time = end - cycle_start\n",
    "    elapsed = end - start\n",
    "#     print('Cycle took: {}'.format(cycle_time))\n",
    "#     print('Total time elapsed: {}'.format(elapsed))\n",
    "\n",
    "#Add Legend and Descriptive Title/Axis Labels\n",
    "plt.legend(bbox_to_anchor=(1,1))\n",
    "plt.ylim(0, 0.01)\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('Number of Features in Model')\n",
    "plt.title('MSE versus Number of Features Incorporated into Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
